{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfYFhojQDxpSuf0B7kaiW8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StaniszewskiA/Loss-Functions/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmOwr2HSRHl1",
        "outputId": "6bd77474-04b1-4da0-d690-9211ae1d9cb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from typing import Dict\n",
        "import torch\n",
        "from torch import optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "def get_config_cnnh_cifar():\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda:0\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    config = {\n",
        "        \"hash_save_path\": \"save/CNNH/\",\n",
        "        \"optimizer\": {\n",
        "            \"type\": optim.Adam,\n",
        "            \"optim_params\": {\n",
        "                \"lr\": 1e-4,\n",
        "                \"betas\": (0.9, 0.999)\n",
        "            }\n",
        "        },\n",
        "        \"info\": \"[CNNH]\",\n",
        "        \"resize_size\": 64,\n",
        "        \"crop_size\": 32,\n",
        "        \"batch_size\": 8,\n",
        "        \"net\": \"ResNet\",\n",
        "        \"dataset\": \"cifar\",\n",
        "        \"epochs\": 100,\n",
        "        \"save_interval\": 10,\n",
        "        \"test_MAP\": 10,\n",
        "        \"device\": device,\n",
        "        \"bit_list\": [12],\n",
        "    }\n",
        "\n",
        "    return config\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(64),\n",
        "    transforms.CenterCrop(32),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "cifar_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(cifar_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
        "\n",
        "bits = 12\n",
        "n = 10000\n",
        "train_labels = torch.tensor([], dtype=torch.long)\n",
        "\n",
        "for data, labels in train_loader:\n",
        "    train_labels = torch.cat((train_labels, labels), dim=0)\n",
        "\n",
        "train_labels_capped = train_labels[:3]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torchvision import models\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "      def __init__(self, hash_bits: int, pretrained: bool = True):\n",
        "        super(AlexNet, self).__init__()\n",
        "\n",
        "        model_alexnet = models.alexnet(pretrained=pretrained)\n",
        "        self.features = model_alexnet.features\n",
        "\n",
        "        self.hash_layer = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, hash_bits),\n",
        "        )\n",
        "\n",
        "      def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        hash_code = self.hash_layer(x)\n",
        "        return hash_code"
      ],
      "metadata": {
        "id": "l9mgy9Sn4w2e"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torchvision import models\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    \"\"\"ResNet for Hashing\"\"\"\n",
        "    def __init__(self, hash_bits: int, pretrained: bool = True):\n",
        "        \"\"\"Constructor\"\"\"\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        model_resnet = models.resnet18(pretrained=pretrained)\n",
        "        self.features = nn.Sequential(*list(model_resnet.children())[:-1])\n",
        "\n",
        "        self.hash_layer = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(model_resnet.fc.in_features, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, hash_bits),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward pass\"\"\"\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        hash_code = self.hash_layer(x)\n",
        "        return hash_code"
      ],
      "metadata": {
        "id": "_KLqoMSC5TD0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_hash(n, bits):\n",
        "  hashes = 2 * torch.rand((n, bits)) - 1\n",
        "  return hashes"
      ],
      "metadata": {
        "id": "RsdqxzaX35lJ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def initialize_sim_matrix(bits):\n",
        "  sim_matrix = np.outer(train_labels_capped, train_labels_capped)\n",
        "\n",
        "  sim_matrix_scaled = np.kron(sim_matrix, np.ones((bits, bits)))\n",
        "\n",
        "  return sim_matrix_scaled"
      ],
      "metadata": {
        "id": "NF84mqeL9pTG"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_hash_matrix(hash):\n",
        "  hash_numpy = np.array(hash)\n",
        "\n",
        "  if len(hash_numpy.shape) == 1:\n",
        "      hash_transposed = hash_numpy.reshape(-1, 1)\n",
        "  else:\n",
        "      hash_transposed = hash_numpy.T\n",
        "\n",
        "  hash_matrix = np.outer(hash_numpy, hash_transposed)\n",
        "\n",
        "  return hash_matrix\n"
      ],
      "metadata": {
        "id": "Fg1C5eOH3J9G"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def reconstruction_triplet_loss(hash_matrix, sim_matrix, triplets):\n",
        "  hash_tensor = torch.tensor(hash_matrix, dtype=torch.float32)\n",
        "  sim_tensor = torch.tensor(sim_matrix, dtype=torch.float32)\n",
        "\n",
        "  reconstruction_loss = torch.nn.functional.mse_loss(hash_tensor, sim_tensor)\n",
        "\n",
        "  anchor, positive, negative = zip(*triplets)\n",
        "\n",
        "  anchor = torch.tensor(anchor, dtype=torch.float32)\n",
        "  positive = torch.tensor(positive, dtype=torch.float32)\n",
        "  negative = torch.tensor(negative, dtype=torch.float32)\n",
        "\n",
        "  pos_distances = F.pairwise_distance(anchor, positive)\n",
        "  neg_distances = F.pairwise_distance(anchor, negative)\n",
        "\n",
        "  margin = 1.0\n",
        "\n",
        "  triplet_loss = F.relu(pos_distances - neg_distances + margin)\n",
        "  #triplet_loss += sim_matrix[torch.arange(len(triplets)), torch.arange(len(triplets))]\n",
        "\n",
        "  alpha = 0.3\n",
        "\n",
        "  loss = alpha * reconstruction_loss + (1 - alpha) * triplet_loss\n",
        "\n",
        "  return loss\n"
      ],
      "metadata": {
        "id": "OPldzmF44Xxp"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNHTrainer:\n",
        "  def __init__(self, cnnh_config) -> None:\n",
        "      self.config = cnnh_config\n",
        "      self.net = self.initialize_model()\n",
        "      self.optimizer = self.setup_optimizer()\n",
        "      self.epochs = self.config[\"epochs\"]\n",
        "      self.train_loader = train_loader\n",
        "      self.hashes = initialize_hash(10000, self.config[\"bit_list\"][0])\n",
        "      self.sim_matrix = initialize_sim_matrix(self.config[\"bit_list\"][0])\n",
        "      self.criterion = reconstruction_triplet_loss\n",
        "\n",
        "  def initialize_model(self):\n",
        "    # Check if the net is a string, and if so, instantiate the corresponding model class\n",
        "    if isinstance(self.config[\"net\"], str):\n",
        "        # You might need to adjust this part based on your available model classes\n",
        "        if self.config[\"net\"] == \"ResNet\":\n",
        "            return ResNet(self.config[\"bit_list\"][0])\n",
        "        elif self.config[\"net\"] == \"AlexNet\":\n",
        "            return AlexNet(self.config[\"bit_list\"][0])\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported network type: {self.config['net']}\")\n",
        "    else:\n",
        "        # If net is already an instantiated model, return it as is\n",
        "        return self.config[\"net\"]\n",
        "\n",
        "\n",
        "  def setup_optimizer(self):\n",
        "        optimizer_type = self.config[\"optimizer\"][\"type\"]\n",
        "        optimizer_params = self.config[\"optimizer\"][\"optim_params\"]\n",
        "\n",
        "        optimizer = optimizer_type(self.net.parameters(), **optimizer_params)\n",
        "\n",
        "        if 'weight_decay' in optimizer_params and optimizer_params['weight_decay'] > 0:\n",
        "            optimizer_params[\"weight_decay\"] = 1e-5\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "  def train(self):\n",
        "    train_losses = []\n",
        "\n",
        "    for epoch in range(self.epochs):\n",
        "        self.net.train()\n",
        "        train_loss = self.train_epoch()\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        if (epoch + 1) % self.config[\"test_MAP\"] == 0:\n",
        "            current_map = self.evaluate()\n",
        "            if current_map > best_map:\n",
        "                best_map = current_map\n",
        "\n",
        "        if (epoch + 1) % self.config[\"save_interval\"] == 0:\n",
        "            self.save_model_state(epoch + 1, best_map, train_losses)\n",
        "\n",
        "        if (epoch + 1) == self.epochs:\n",
        "            self.evaluate()\n",
        "            self.plot_results()\n",
        "\n",
        "  def train_epoch(self):\n",
        "    epoch_losses = []\n",
        "    for batch_index, (images, labels) in enumerate(self.train_loader):\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        predictions = self.net(images)\n",
        "\n",
        "        # Generate triplets for the current batch\n",
        "        batch_triplets = generate_triplets(self.hashes)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = self.calculate_loss(predictions, batch_triplets)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        self.optimizer.step()\n",
        "\n",
        "        epoch_losses.append(loss.item())\n",
        "\n",
        "    return epoch_losses\n",
        "\n",
        "  def calculate_loss(self, prediction, targets):\n",
        "    hash_matrix = compute_hash_matrix(self.hashes)\n",
        "    sim_matrix = self.sim_matrix\n",
        "\n",
        "    hash_tensor = torch.tensor(hash_matrix, dtype=torch.float32)\n",
        "    sim_tensor = torch.tensor(sim_matrix, dtype=torch.float32)\n",
        "\n",
        "    reconstruction_loss = F.mse_loss(hash_tensor, sim_tensor)\n",
        "\n",
        "    anchor, positive, negative = zip(*triplets)\n",
        "\n",
        "    anchor = torch.tensor(anchor, dtype=torch.float32)\n",
        "    positive = torch.tensor(positive, dtype=torch.float32)\n",
        "    negative = torch.tensor(negative, dtype=torch.float32)\n",
        "\n",
        "    pos_distances = F.pairwise_distance(anchor, positive)\n",
        "    neg_distances = F.pairwise_distance(anchor, negative)\n",
        "\n",
        "    margin = 1.0\n",
        "    triplet_loss = F.relu(pos_distances - neg_distances + margin)\n",
        "\n",
        "    alpha = 0.3\n",
        "    loss = alpha * reconstruction_loss + (1 - alpha) * triplet_loss\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "  def save_model_state(self, epoch, best_map, train_losses):\n",
        "    bit = self.config[\"bit_list\"]\n",
        "    save_path = f\"model_state_epoch_{epoch}_{self.config['dataset']}_{bit}.pt\"\n",
        "    torch.save(self.net.state_dict(), save_path)\n",
        "    print(f\"Model state saved at epoch {epoch}: {save_path}\")\n",
        "\n",
        "    result_dict = {\n",
        "        \"epoch\": epoch,\n",
        "        \"best_map\": best_map,\n",
        "        \"train_losses\": train_losses\n",
        "    }\n",
        "    result_path = f\"training_results_epoch_{epoch}.json\"\n",
        "    with open(result_path, 'w') as result_file:\n",
        "        json.dump(result_dict, result_file)\n",
        "    print(f\"Training results saved at epoch {epoch}: {result_path}\")"
      ],
      "metadata": {
        "id": "yXNJbkd66RRx"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_triplets(hashes):\n",
        "  triplets = []\n",
        "\n",
        "  for i in range(len(hashes)):\n",
        "      anchor = hashes[i]\n",
        "\n",
        "      # Select a positive sample (similar to the anchor)\n",
        "      positive_index = np.random.choice(np.where(hashes == anchor)[0])\n",
        "      positive = hashes[positive_index]\n",
        "\n",
        "      # Select a negative sample (dissimilar to the anchor)\n",
        "      negative_index = np.random.choice(np.where(hashes != anchor)[0])\n",
        "      negative = hashes[negative_index]\n",
        "\n",
        "      triplets.append((anchor, positive, negative))\n",
        "\n",
        "  return triplets"
      ],
      "metadata": {
        "id": "V9qly1gaGOjd"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnnh_config_cifar = get_config_cnnh_cifar()\n",
        "\n",
        "bits = 1\n",
        "data_amount = train_labels_capped.size(0)\n",
        "\n",
        "hashes = initialize_hash(data_amount, bits)\n",
        "hash_matrix = compute_hash_matrix(hashes)\n",
        "sim_matrix = initalize_sim_matrix(bits)\n",
        "\n",
        "print(\"Hash Matrix:\")\n",
        "print(hash_matrix)\n",
        "print(\"\\nSimilarity Matrix:\")\n",
        "print(sim_matrix)\n",
        "\n",
        "triplets = generate_triplets(hashes)\n",
        "\n",
        "rec_loss = reconstruction_triplet_loss(hash_matrix, sim_matrix, triplets)\n",
        "\n",
        "print(rec_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ug4Yj2wh1b2N",
        "outputId": "ad9d3a25-6d1f-40a1-89d6-925ae4702d56"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hash Matrix:\n",
            "[[ 0.59496176 -0.23058371 -0.06509267]\n",
            " [-0.23058371  0.08936516  0.02522735]\n",
            " [-0.06509267  0.02522735  0.00712156]]\n",
            "\n",
            "Similarity Matrix:\n",
            "[[25. 45. 10.]\n",
            " [45. 81. 18.]\n",
            " [10. 18.  4.]]\n",
            "tensor(403.2830)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnnh_config_cifar = get_config_cnnh_cifar()\n",
        "print(cnnh_config_cifar)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "cnnh_trainer = CNNHTrainer(cnnh_config_cifar)\n",
        "cnnh_trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5SIdR8-Y8mH",
        "outputId": "f9f7f57d-ad14-4e79-d8f4-92d3ae7e5f31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hash_save_path': 'save/CNNH/', 'optimizer': {'type': <class 'torch.optim.adam.Adam'>, 'optim_params': {'lr': 0.0001, 'betas': (0.9, 0.999)}}, 'info': '[CNNH]', 'resize_size': 64, 'crop_size': 32, 'batch_size': 8, 'net': 'ResNet', 'dataset': 'cifar', 'epochs': 100, 'save_interval': 10, 'test_MAP': 10, 'device': device(type='cpu'), 'bit_list': [12]}\n"
          ]
        }
      ]
    }
  ]
}